{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgo5pGCuq83O"
      },
      "source": [
        "# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY1PtZG6rAem"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1AR2fseeE02BLvKP79AIutmBumUDTnLM8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynSV3tBxLF6e"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1ZdDOxtPvfGbXXX-6r3h0CSsTiPR0N1M5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw27Jv_ILLHn"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1fF13ltpNqEzCn23CE8eDGDpAsxVX2q6p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NOFjO_bLLog"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1Ze2WODE7wG9uOh2UrCwgNYh38EivSDg4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzEEC16Fr2ib"
      },
      "source": [
        "\n",
        "Source: https://www.kaggle.com/c/facial-keypoints-detection/overview\n",
        "\n",
        "Source: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40uh1KD1sp6O"
      },
      "source": [
        "# PART 1. FACIAL KEY POINTS DETECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_bvxR_vLLje"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1FRzH6TjQzGAlhfFaGe2FNTW9d37ReAKW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or9DLFyrLLfK"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1zYoscqhnB8Fv1qbpVdJe9_rTIqSmFkyW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTymJearEJn"
      },
      "source": [
        "# TASK #2: IMPORT LIBRARIES AND DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXQvU_ijrFp8"
      },
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shell command will set the present working directory to,\n",
        "# %cd /content/drive/MyDrive/Github"
      ],
      "metadata": {
        "id": "yLOzFUNCoh1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkdq6xgwrHD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01107e7c-243f-4949-cd72-16b7407960b3"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Modern AI Portfolio Builder/Emotion AI Dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Modern AI Portfolio Builder/Emotion AI Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQFbssqfr_UL"
      },
      "source": [
        "# Import the necessary packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from PIL import *\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.python.keras import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klj2F-OQr_Wn"
      },
      "source": [
        "# load facial key points data\n",
        "keyfacial_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Modern AI Portfolio Builder/Emotion AI Dataset/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTb3fLL8r_ZQ"
      },
      "source": [
        "keyfacial_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yFsN8SAtHum"
      },
      "source": [
        "# Obtain relavant information about the dataframe\n",
        "keyfacial_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJhdlYRqtKVK"
      },
      "source": [
        "# Check if null values exist in the dataframe\n",
        "keyfacial_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-s17cxKBvd3"
      },
      "source": [
        "keyfacial_df['Image'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtGMT3ozr_g3"
      },
      "source": [
        "# Since values for the image are given as space separated string, separate the values using ' ' as separator.\n",
        "# Then convert this into numpy array using np.fromstring and convert the obtained 1D array into 2D array of shape (96, 96)\n",
        "keyfacial_df['Image'] = keyfacial_df['Image'].apply(lambda x: np.fromstring(x, dtype = int, sep = ' ').reshape(96, 96))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1RIIy8r_fN"
      },
      "source": [
        "# Obtain the Shape of the image\n",
        "keyfacial_df['Image'][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkADotpCdIfW"
      },
      "source": [
        "MINI CHALLENGE #1:\n",
        "- Obtain the average, minimum and maximum values for 'right_eye_center_x' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06r0LqnKflxU"
      },
      "source": [
        "keyfacial_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNkYttG_tOKf"
      },
      "source": [
        "# TASK #3: PERFORM IMAGE VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJujwjyr_dc"
      },
      "source": [
        "# Plot a random image from the dataset along with facial keypoints. \n",
        "# Image data is obtained from df['Image'] and plotted using plt.imshow\n",
        "# 15 x and y coordinates for the corresponding image \n",
        "# since x-coordinates are in even columns like 0,2,4,.. and y-coordinates are in odd columns like 1,3,5,..\n",
        "# we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.\n",
        "\n",
        "i = np.random.randint(1, len(keyfacial_df))\n",
        "plt.imshow(keyfacial_df['Image'][i], cmap = 'gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSZfQwc3r_be"
      },
      "source": [
        "# Let's view more images in a grid format\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(16):\n",
        "    ax = fig.add_subplot(4, 4, i + 1)    \n",
        "    image = plt.imshow(keyfacial_df['Image'][i],cmap = 'gray')\n",
        "    for j in range(1,31,2):\n",
        "        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBJERZH_fzg0"
      },
      "source": [
        "MINI CHALLENGE #2: \n",
        "- Perform a sanity check on the data by randomly visualizing 64 new images along with their cooresponding key points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPbtjUqHhYTX"
      },
      "source": [
        "import random\n",
        "# Let's view more images in a grid format\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(64):\n",
        "    k = random.randint(1, len(keyfacial_df))\n",
        "    ax = fig.add_subplot(8, 8, i + 1)    \n",
        "    image = plt.imshow(keyfacial_df['Image'][k],cmap = 'gray')\n",
        "    for j in range(1,31,2):\n",
        "        plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], 'rx')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbqDwd1mteJ4"
      },
      "source": [
        "# TASK #4: PERFORM IMAGE AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3TLq1UbtazX"
      },
      "source": [
        "# Create a new copy of the dataframe\n",
        "import copy\n",
        "keyfacial_df_copy = copy.copy(keyfacial_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypyn10X7tbrb"
      },
      "source": [
        "# Obtain the columns in the dataframe\n",
        "\n",
        "columns = keyfacial_df_copy.columns[:-1]\n",
        "columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyFb3o1ztbyr"
      },
      "source": [
        "# Horizontal Flip - flip the images along y axis\n",
        "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 1))\n",
        "\n",
        "# since we are flipping horizontally, y coordinate values would be the same\n",
        "# Only x coordiante values would change, all we have to do is to subtract our initial x-coordinate values from width of the image(96)\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 0:\n",
        "    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIM1786rtb37"
      },
      "source": [
        "# Show the Original image\n",
        "plt.imshow(keyfacial_df['Image'][0], cmap = 'gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df.loc[0][j-1], keyfacial_df.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVUp-UMtbwg"
      },
      "source": [
        "# Show the Horizontally flipped image\n",
        "plt.imshow(keyfacial_df_copy['Image'][0],cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g67tKjwtbua"
      },
      "source": [
        "# Concatenate the original dataframe with the augmented dataframe\n",
        "augmented_df = np.concatenate((keyfacial_df, keyfacial_df_copy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRag8bteDzVl"
      },
      "source": [
        "augmented_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COzWKpthtvRW"
      },
      "source": [
        "# Randomingly increasing the brightness of the images\n",
        "# We multiply pixel values by random values between 1.5 and 2 to increase the brightness of the image\n",
        "# we clip the value between 0 and 255\n",
        "\n",
        "import random\n",
        "\n",
        "keyfacial_df_copy = copy.copy(keyfacial_df)\n",
        "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x:np.clip(random.uniform(1.5, 2)* x, 0.0, 255.0))\n",
        "augmented_df = np.concatenate((augmented_df, keyfacial_df_copy))\n",
        "augmented_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Pvd2qqtvkF"
      },
      "source": [
        "# Show Image with increased brightness\n",
        "\n",
        "plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm0qhrGNdcY9"
      },
      "source": [
        "MINI CHALLENGE #3:\n",
        "- Augment images by flipping them vertically \n",
        "(Hint: Flip along x-axis and note that if we are flipping along x-axis, x co-ordinates won't change)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQV4Lwr6dkmK"
      },
      "source": [
        "keyfacial_df_copy = copy.copy(keyfacial_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X607yQkqCBd9"
      },
      "source": [
        "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 0))\n",
        "\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 1:\n",
        "    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olFC4fPRixLm"
      },
      "source": [
        "MINI CHALLENGE #4:\n",
        "- Perform a sanity check and visualize sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERhDcD2XiP4u"
      },
      "source": [
        "plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv4RFiu4iP2c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz56c0e0t71Y"
      },
      "source": [
        "# TASK #5: PERFORM DATA NORMALIZATION AND TRAINING DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNdOOb0Ctvei"
      },
      "source": [
        "# Obtain the value of images which is present in the 31st column (since index start from 0, we refer to 31st column by 30)\n",
        "img = augmented_df[:,30]\n",
        "\n",
        "# Normalize the images\n",
        "img = img/255.\n",
        "\n",
        "# Create an empty array of shape (x, 96, 96, 1) to feed the model\n",
        "X = np.empty((len(img), 96, 96, 1))\n",
        "\n",
        "# Iterate through the img list and add image values to the empty array after expanding it's dimension from (96, 96) to (96, 96, 1)\n",
        "for i in range(len(img)):\n",
        "  X[i,] = np.expand_dims(img[i], axis = 2)\n",
        "\n",
        "# Convert the array type to float32\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPNN-RKftvcF"
      },
      "source": [
        "# Obtain the value of x & y coordinates which are to used as target.\n",
        "y = augmented_df[:,:30]\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl7FHUUMtvZr"
      },
      "source": [
        "# Split the data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN2fmPwrd4CZ"
      },
      "source": [
        "MINI CHALLENGE #5: \n",
        "- Try a different value for 'test_size' and verify that the split was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQmZG5fxkU9d"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe_63C3PFiIC"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO5Gj9SuVsNl"
      },
      "source": [
        "# TASK #6: UNDERSTAND THE THEORY AND INTUITION BEHIND NEURAL NETWORKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyiKUgs5LLTY"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1k-oQ76AJGcmCNEvLclSncNLqIti-TzjP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB0J_w0pLLQL"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1c_dyUkjlMGBVTUyU8TqzcuACP_deZEZX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l82oD8xWktf4"
      },
      "source": [
        "MINI CHALLENGE #6: \n",
        "- List at least 3 types of activation functions, which type is preferred in the hidden layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqewaqxuoAki"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBZyZeP9MVy6"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1e0WgA3w_gXuGsdtJMk5oOMjNUhINHN1-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVjBgYLRoBOC"
      },
      "source": [
        "MINI CHALLENGE #7: \n",
        "- List at least 3 different neural networks architectures and their applications. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8I3pVH-ks1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd5FRPK0XYXJ"
      },
      "source": [
        "# TASK #7: UNDERSTAND NEURAL NETWORKS TRAINING PROCESS AND GRADIENT DESCENT ALGORITHM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYPgtg5cMVvU"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1dzeIVkRaMUs6U-PX0YBWivqYH2w6eBjA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW-KOTwPMVsw"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1NDKi1VCGWsICFDqdWA23FVv7Dx9Ev1dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOy1_uEUMVp6"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1t9KvJSFby5EAq5T7q83mLLGMMYHT7Wec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTG3nOqCMVnJ"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1nDbmW2GfC5iqeBtbo5gspq1sE99Oyps5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftQV1zyVM3yf"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1SiC-Z2gZAMk6cnhh6OdSQDQyH7YKH6ub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIwlzHjym4zj"
      },
      "source": [
        "MINI CHALLENGE #8: \n",
        "- What happen when you set the learning rate to the extremes (i.e.: very small value and very large value)? How can you achieve the best of both worlds?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40cFtAm3na__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZKldSlZBNG"
      },
      "source": [
        "# TASK #8: UNDERSTAND THE THEORY AND INTUITION BEHIND CONVOLUTIONAL NEURAL NETWORKS AND RESNETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpGUMxp0M3uq"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1q0If_tsIyz2GfOb9EsRH_8_ue-1QnTlu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kUdHq-LM3sE"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=15qVaiDTzm763kFmh0ayARH4FSN2MUsgc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRhvF8H3M3pE"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1G_gvfoKJyRGpq_oNejLwDj7ZxIZvCGk6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRSSNhBM3l2"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1GDhehqRRtnTA3-i02cYcalbPA27ej7Ar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_z82VFtoSy"
      },
      "source": [
        "MINI CHALLENGE #9: \n",
        "- What's the performance of ResNets compared to AlexNet in ImageNet dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP2G_du-tnlV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIXkiaHBuNrg"
      },
      "source": [
        "# TASK #9: BUILD DEEP RESIDUAL NEURAL NETWORK KEY FACIAL POINTS DETECTION MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCAPjVotvXi"
      },
      "source": [
        "def res_block(X, filter, stage):\n",
        "\n",
        "  # Convolutional_block\n",
        "  X_copy = X\n",
        "\n",
        "  f1 , f2, f3 = filter\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = MaxPool2D((2,2))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n",
        "\n",
        "\n",
        "  # Short path\n",
        "  X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n",
        "  X_copy = MaxPool2D((2,2))(X_copy)\n",
        "  X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  # Identity Block 1\n",
        "  X_copy = X\n",
        "\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  # Identity Block 2\n",
        "  X_copy = X\n",
        "\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB9dPqlTtvVH"
      },
      "source": [
        "input_shape = (96, 96, 1)\n",
        "\n",
        "# Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "# Zero-padding\n",
        "X = ZeroPadding2D((3,3))(X_input)\n",
        "\n",
        "# 1 - stage\n",
        "X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((3,3), strides= (2,2))(X)\n",
        "\n",
        "# 2 - stage\n",
        "X = res_block(X, filter= [64,64,256], stage= 2)\n",
        "\n",
        "# 3 - stage\n",
        "X = res_block(X, filter= [128,128,512], stage= 3)\n",
        "\n",
        "\n",
        "# Average Pooling\n",
        "X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n",
        "\n",
        "# Final layer\n",
        "X = Flatten()(X)\n",
        "X = Dense(4096, activation = 'relu')(X)\n",
        "X = Dropout(0.2)(X)\n",
        "X = Dense(2048, activation = 'relu')(X)\n",
        "X = Dropout(0.1)(X)\n",
        "X = Dense(30, activation = 'relu')(X)\n",
        "\n",
        "\n",
        "model_1_facialKeyPoints = Model( inputs= X_input, outputs = X)\n",
        "model_1_facialKeyPoints.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEND_oH7mANd"
      },
      "source": [
        "MINI CHALLENGE #10:\n",
        "- Experiment with changing the network architecture by removing 2 MaxPooling layers from the Res Block and train the model\n",
        "- Try to add 'X = res_block(X, filter= [256,256,1024], stage= 4)' Block after stage #3 block. \n",
        "- What did you observe? Comment on your answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6yZkdjLeFnN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJCLhas0ulnq"
      },
      "source": [
        "# TASK #10: COMPILE AND TRAIN KEY FACIAL POINTS DETECTION DEEP LEARNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr662N98upOP"
      },
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
        "model_1_facialKeyPoints.compile(loss = \"mean_squared_error\", optimizer = adam , metrics = ['accuracy'])\n",
        "# Check this out for more information on Adam optimizer: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pj-gfDCusny"
      },
      "source": [
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"FacialKeyPoints_weights.hdf5\", verbose = 1, save_best_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvWHFJyousme"
      },
      "source": [
        "history = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32, epochs = 2, validation_split = 0.05, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VLk1ZKPusjM"
      },
      "source": [
        "# save the model architecture to json file for future use\n",
        "\n",
        "model_json = model_1_facialKeyPoints.to_json()\n",
        "with open(\"FacialKeyPoints-model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_p7ramEeMGa"
      },
      "source": [
        "MINI CHALLENGE #11:\n",
        "- Experiment with changing the batch size and validation split value and retrain the model (Take Home)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GynkMge9eNd_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-cVW8K62Pai"
      },
      "source": [
        "# TASK #11: ASSESS TRAINED KEY FACIAL POINTS DETECTION MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iGWmYbdu2mF"
      },
      "source": [
        "with open('detection.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "    \n",
        "# load the model architecture \n",
        "model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_1_facialKeyPoints.load_weights('weights_keypoint.hdf5')\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_1_facialKeyPoints.compile(loss=\"mean_squared_error\", optimizer= adam , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XuTiBexu2j5"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "result = model_1_facialKeyPoints.evaluate(X_test, y_test)\n",
        "print(\"Accuracy : {}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA-LZmf-u2iE"
      },
      "source": [
        "# Get the model keys \n",
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJfpDL2xu2cV"
      },
      "source": [
        "# Plot the training artifacts\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','val_loss'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvTWqbOI2tty"
      },
      "source": [
        "# PART 2. FACIAL EXPRESSION DETECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhqN36LmLLav"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1Gh1m53Gi7AGDVKAxFENEdP7067fLmK0-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IThnROSSLLXA"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=18v6rj-1xyy9xzbIQ2N2SK8MbnmoH1oKE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIKg0ZXk3DWl"
      },
      "source": [
        "# TASK #12: IMPORT & EXPLORE DATASET FOR FACIAL EXPRESSION DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NT8ASSu2aT"
      },
      "source": [
        "# read the csv files for the facial expression data\n",
        "facialexpression_df = pd.read_csv('icml_face_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTiebNB2u2YP"
      },
      "source": [
        "facialexpression_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGIesU_F0AMm"
      },
      "source": [
        "facialexpression_df[' pixels'][0] # String format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMLQbIEh3Ma6"
      },
      "source": [
        "# function to convert pixel values in string format to array format\n",
        "\n",
        "def string2array(x):\n",
        "  return np.array(x.split(' ')).reshape(48, 48, 1).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkUHXFoU3MYq"
      },
      "source": [
        "# Resize images from (48, 48) to (96, 96)\n",
        "\n",
        "def resize(x):\n",
        "  \n",
        "  img = x.reshape(48, 48)\n",
        "  return cv2.resize(img, dsize=(96, 96), interpolation = cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GexILbbK3MWK"
      },
      "source": [
        "facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: string2array(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZr0wp553MUp"
      },
      "source": [
        "facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: resize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rbv7ZyC3MSM"
      },
      "source": [
        "facialexpression_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkreBNCT3MPu"
      },
      "source": [
        "# check the shape of data_frame\n",
        "facialexpression_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkQQp5gy3MNa"
      },
      "source": [
        "# check for the presence of null values in the data frame\n",
        "facialexpression_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-BzW6jL3MK0"
      },
      "source": [
        "label_to_text = {0:'anger', 1:'disgust', 2:'sad', 3:'happiness', 4: 'surprise'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ceohXF_1QIk"
      },
      "source": [
        "MINI CHALLENGE #12: \n",
        "- Visualize the first image in the dataframe and make sure that the image is not distorted by resizing or reshaping operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xm9oL5k1ED6"
      },
      "source": [
        "plt.imshow(facialexpression_df[' pixels'][0], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JtBq4Ys3m5A"
      },
      "source": [
        "# TASK #13: VISUALIZE IMAGES AND PLOT LABELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZmIbWvE3rJM"
      },
      "source": [
        "emotions = [0, 1, 2, 3, 4]\n",
        "\n",
        "for i in emotions:\n",
        "  data = facialexpression_df[facialexpression_df['emotion'] == i][:1]\n",
        "  img = data[' pixels'].item()\n",
        "  img = img.reshape(96, 96)\n",
        "  plt.figure()\n",
        "  plt.title(label_to_text[i])\n",
        "  plt.imshow(img, cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQj1t3mt1t7_"
      },
      "source": [
        "MINI CHALLENGE #13: \n",
        "- Plot bar chart to outline how many samples (images) are present per emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqd57PASp_tl"
      },
      "source": [
        "facialexpression_df.emotion.value_counts().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMnwhXw_qGyq"
      },
      "source": [
        "facialexpression_df.emotion.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxAbaC1g3pNP"
      },
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib_YTklF3wGy"
      },
      "source": [
        "# TASK #14: PERFORM DATA PREPARATION AND IMAGE AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgzxfltG3rHt"
      },
      "source": [
        "# split the dataframe in to features and labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "X = facialexpression_df[' pixels']\n",
        "y = to_categorical(facialexpression_df['emotion'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx91bryj0zu_"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNDkM9ZL0-kP"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfPslLye0wgZ"
      },
      "source": [
        "\n",
        "X = np.stack(X, axis = 0)\n",
        "X = X.reshape(24568, 96, 96, 1)\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1zcH-d83rGF"
      },
      "source": [
        "# split the dataframe in to train, test and validation data frames\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_Test, y_train, y_Test = train_test_split(X, y, test_size = 0.1, shuffle = True)\n",
        "X_val, X_Test, y_val, y_Test = train_test_split(X_Test, y_Test, test_size = 0.5, shuffle = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLQt43f3rE2"
      },
      "source": [
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Febj9BnO1xaA"
      },
      "source": [
        "print(X_Test.shape, y_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDH4-6h211dj"
      },
      "source": [
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDAFSti14MUD"
      },
      "source": [
        "# image pre-processing\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val   = X_val /255\n",
        "X_Test  = X_Test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh8hASOZ1_GG"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IgiiGW64MRm"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "rotation_range = 15,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    shear_range = 0.1,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQArvQk23k2g"
      },
      "source": [
        "MINI CHALLENGE #14: \n",
        "- Try at least 2 additional data augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NyS_hKR3j_I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5yyM1UF4Xv2"
      },
      "source": [
        "# TASK #15: BUILD AND TRAIN DEEP LEARNING MODEL FOR FACIAL EXPRESSION CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iYII6C24MQD"
      },
      "source": [
        "input_shape = (96, 96, 1)\n",
        "\n",
        "# Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "# Zero-padding\n",
        "X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "# 1 - stage\n",
        "X = Conv2D(64, (7, 7), strides= (2, 2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((3, 3), strides= (2, 2))(X)\n",
        "\n",
        "# 2 - stage\n",
        "X = res_block(X, filter= [64, 64, 256], stage= 2)\n",
        "\n",
        "# 3 - stage\n",
        "X = res_block(X, filter= [128, 128, 512], stage= 3)\n",
        "\n",
        "# 4 - stage\n",
        "# X = res_block(X, filter= [256, 256, 1024], stage= 4)\n",
        "\n",
        "# Average Pooling\n",
        "X = AveragePooling2D((4, 4), name = 'Averagea_Pooling')(X)\n",
        "\n",
        "# Final layer\n",
        "X = Flatten()(X)\n",
        "X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n",
        "\n",
        "model_2_emotion = Model( inputs= X_input, outputs = X, name = 'Resnet18')\n",
        "\n",
        "model_2_emotion.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SHnD2aX4MLJ"
      },
      "source": [
        "# train the network\n",
        "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ubSY9CQ4MJG"
      },
      "source": [
        "# Recall that the first facial key points model was saved as follows: FacialKeyPoints_weights.hdf5 and FacialKeyPoints-model.json\n",
        "\n",
        "# using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)\n",
        "\n",
        "# save the best model with lower validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"FacialExpression_weights.hdf5\", verbose = 1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjpKgHK3q_n"
      },
      "source": [
        "history = model_2_emotion.fit(train_datagen.flow(X_train, y_train, batch_size=64),\n",
        "\tvalidation_data=(X_val, y_val), steps_per_epoch=len(X_train) // 64,\n",
        "\tepochs= 2, callbacks=[checkpointer, earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJe9up3A4zUI"
      },
      "source": [
        "# saving the model architecture to json file for future use\n",
        "\n",
        "model_json = model_2_emotion.to_json()\n",
        "with open(\"FacialExpression-model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS71oV-j2ZXh"
      },
      "source": [
        "MINI CHALLENGE #15: \n",
        "- Experiment with various batch size, patience, optimizers, and network architecture to improve network performance (Take home)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MREPeVkF2Y3i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrkNZ8Yua2J_"
      },
      "source": [
        "# TASK #16: UNDERSTAND HOW TO ASSESS CLASSIFIER MODELS (CONFUSION MATRIX, ACCURACY, PRECISION, AND RECALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMw9HpK9NZ8y"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1__5rAZ0LoQJzTkPKIaMmspeBY9CEB_YE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvzzVpohNZ58"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1EDT2wThkKiH2WRuOVVXf_n_Jixb27BjB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK6scQQZNZ3a"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1SwXf4hlCLEJVbAI8H4tcDO5FCEGf7h1o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LxpMt2h5AIi"
      },
      "source": [
        "# TASK #17: ASSESS THE PERFORMANCE OF TRAINED FACIAL EXPRESSION CLASSIFIER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtc94LP4zSj"
      },
      "source": [
        "with open('emotion.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "    \n",
        "# load the model architecture \n",
        "model_2_emotion = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_2_emotion.load_weights('weights_emotions.hdf5')\n",
        "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoYkZWw43zv"
      },
      "source": [
        "score = model_2_emotion.evaluate(X_Test, y_Test)\n",
        "print('Test Accuracy: {}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M27CZcLm4zQk"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2WZ3__e5Kxp"
      },
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEO2vCRh5Kpz"
      },
      "source": [
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9WS5-Bv5KoD"
      },
      "source": [
        "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oduf0Q8n5Kl-"
      },
      "source": [
        "# predicted_classes = model.predict_classes(X_test)\n",
        "predicted_classes = np.argmax(model_2_emotion.predict(X_Test), axis=-1)\n",
        "y_true = np.argmax(y_Test, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Yp58fm5Kkj"
      },
      "source": [
        "y_true.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLdhjhBb5KeD"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, predicted_classes)\n",
        "plt.figure(figsize = (10, 10))\n",
        "sns.heatmap(cm, annot = True, cbar = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTQGje8f6ug0"
      },
      "source": [
        "MINI CHALLENGE #16: \n",
        "- Print out a grid of 25 images along with their predicted/true label\n",
        "- Print out the classification report and analyze precision and recall "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuLsavNIGQqA"
      },
      "source": [
        "L = 5\n",
        "W = 5\n",
        "\n",
        "fig, axes = plt.subplots(L, W, figsize = (24, 24))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in np.arange(0, L*W):\n",
        "    axes[i].imshow(X_test[i].reshape(96,96), cmap = 'gray')\n",
        "    axes[i].set_title('Prediction = {}\\n True = {}'.format(label_to_text[predicted_classes[i]], label_to_text[y_true[i]]))\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace = 1)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yT4LdSCGuDJ"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, predicted_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Gm-Vap7Wv0"
      },
      "source": [
        "# PART 3. COMBINE BOTH FACIAL EXPRESSION AND KEY POINTS DETECTION MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_kdiO4o7IPc"
      },
      "source": [
        "# TASK #18: COMBINE BOTH MODELS (1) FACIAL KEY POINTS DETECTION AND (2) FACIAL EXPRESSION MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDVsSChW7Vpi"
      },
      "source": [
        "def predict(X_test):\n",
        "\n",
        "  # Making prediction from the keypoint model\n",
        "  df_predict = model_1_facialKeyPoints.predict(X_test)\n",
        "\n",
        "  # Making prediction from the emotion model\n",
        "  df_emotion = np.argmax(model_2_emotion.predict(X_test), axis=-1)\n",
        "\n",
        "  # Reshaping array from (856,) to (856,1)\n",
        "  df_emotion = np.expand_dims(df_emotion, axis = 1)\n",
        "\n",
        "  # Converting the predictions into a dataframe\n",
        "  df_predict = pd.DataFrame(df_predict, columns= columns)\n",
        "\n",
        "  # Adding emotion into the predicted dataframe\n",
        "  df_predict['emotion'] = df_emotion\n",
        "\n",
        "  return df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULEWfmhK7vVf"
      },
      "source": [
        "df_predict = predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g1M3Hrq7vO-"
      },
      "source": [
        "df_predict.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmhAnDXf7HpU"
      },
      "source": [
        "MINI CHALLENGE #17: \n",
        "- Plot a grid of 16 images along with their predicted emotion and facial key points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ogvh7t-JhhM"
      },
      "source": [
        "# Plotting the test images and their predicted keypoints and emotions\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize = (24, 24))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(16):\n",
        "\n",
        "    axes[i].imshow(X_test[i].squeeze(),cmap='gray')\n",
        "    axes[i].set_title('Prediction = {}'.format(label_to_text[df_predict['emotion'][i]]))\n",
        "    axes[i].axis('off')\n",
        "    for j in range(1,31,2):\n",
        "            axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHxVJ2Ei8QDP"
      },
      "source": [
        "# PART 4. DEPLOY BOTH TRAINED MODELS "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIm4ZWVu8oKz"
      },
      "source": [
        "# TASK #19. SAVE THE TRAINED MODEL FOR DEPLOYMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxY1AMo0NZ0R"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1bjjMMF3qorxHwW2DWRugsrkf9YKRIaf3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh5Bn3E-N-tt"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1RltAICf4OkErndMnGDC_FB7wh7zuDDWJ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LinXEaAyN-2e"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1p686eR8SxMLkKdS60hY9i1l_G6hXrnND)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q59NUF_d80kJ"
      },
      "source": [
        "- We now need to save our trained model and it has to be saved in a SavedModel format.\n",
        "- The model will have a version number and will be saved in a structured directory \n",
        "- tf.saved_model.save is a function used to build a saved model that is suitable for serving using Tensorflow Serving. \n",
        "- After the model is saved, we can now use TensorFlow Serving to start making inference requests using a specific version of our trained model \"servable\".\n",
        "- Use SavedModel to save and load your model—variables, the graph, and the graph's metadata. \n",
        "          \n",
        "- Check this out for more information: \n",
        "https://www.tensorflow.org/guide/saved_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKhDctJ7vLb"
      },
      "source": [
        "import json\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def deploy(directory, model):\n",
        "  MODEL_DIR = directory\n",
        "  version = 1 \n",
        "\n",
        "  # Let's join the temp model directory with our chosen version number \n",
        "  # The expected result will be = '\\tmp\\version number'\n",
        "  export_path = os.path.join(MODEL_DIR, str(version))\n",
        "  print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "  # Let's save the model using saved_model.save\n",
        "  # If the directory already exists, we will remove it using '!rm' \n",
        "  # rm removes each file specified on the command line. \n",
        "\n",
        "  if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}\n",
        "\n",
        "  tf.saved_model.save(model, export_path)\n",
        "\n",
        "  os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe8T2Oow8-v7"
      },
      "source": [
        "# TASK #20. SERVE THE MODEL USING TENSORFLOW SERVING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZxSvHdm9Iqy"
      },
      "source": [
        "# Let's add tensorflow-model-server package to our list of packages \n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgk_mdNz9j3z"
      },
      "source": [
        "# Let's install tensorflow model server\n",
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0U2XmtK9x1l"
      },
      "source": [
        "# Let's run TensorFlow serving"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUljZE8I9-V_"
      },
      "source": [
        "- We will load our model and start making inferences (predictions) based on it.\n",
        "- There are some important parameters:\n",
        "\n",
        "  - rest_api_port: The port that you'll use for REST requests.\n",
        "  - model_name: You'll use this in the URL of REST requests. You can choose any name\n",
        "  - model_base_path: This is the path to the directory where you've saved your model.\n",
        "  \n",
        "- For more information regarding REST, check this out: \n",
        "https://www.codecademy.com/articles/what-is-rest\n",
        "- REST is a revival of HTTP in which http commands have semantic meaning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDn7Dbrf9x0A"
      },
      "source": [
        "deploy('/model', model_1_facialKeyPoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leau6glq9xyZ"
      },
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=4500 \\\n",
        "  --model_name=keypoint_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgj-FIC9xwI"
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si54Msq0-HBx"
      },
      "source": [
        "deploy('/model1', model_2_emotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEoiUdfG-HAB"
      },
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=4000 \\\n",
        "  --model_name=emotion_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu1wunTw-G-P"
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I5OZyc4-WAl"
      },
      "source": [
        "# TASK #21: MAKE REQUESTS TO MODEL IN TENSORFLOW SERVING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuecWgjz-G8u"
      },
      "source": [
        "import json\n",
        "\n",
        "# Let's create a JSON object and make 3 inference requests\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": X_test[0:3].tolist()})\n",
        "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci5jE7Je-G6a"
      },
      "source": [
        "!pip install -q requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cJz8Nkd-is7"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Function to make predictions from deployed models\n",
        "def response(data):\n",
        "  headers = {\"content-type\": \"application/json\"}\n",
        "  json_response = requests.post('http://localhost:4500/v1/models/keypoint_model/versions/1:predict', data=data, headers=headers, verify = False)\n",
        "  df_predict = json.loads(json_response.text)['predictions']\n",
        "  json_response = requests.post('http://localhost:4000/v1/models/emotion_model/versions/1:predict', data=data, headers=headers, verify = False)\n",
        "  df_emotion = np.argmax(json.loads(json_response.text)['predictions'], axis = 1)\n",
        "  \n",
        "  # Reshaping array from (856,) to (856,1)\n",
        "  df_emotion = np.expand_dims(df_emotion, axis = 1)\n",
        "\n",
        "  # Converting the predictions into a dataframe\n",
        "  df_predict= pd.DataFrame(df_predict, columns = columns)\n",
        "\n",
        "  # Adding emotion into the predicted dataframe\n",
        "  df_predict['emotion'] = df_emotion\n",
        "\n",
        "  return df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV0UAo58-irJ"
      },
      "source": [
        "# making prediction\n",
        "df_predict = response(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrQIJpZg-ipg"
      },
      "source": [
        "df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOTIG6gH-ugH"
      },
      "source": [
        "# Plotting the test images and their predicted keypoints and emotions\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize = (24, 24))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "    axes[i].imshow(X_test[i].squeeze(),cmap='gray')\n",
        "    axes[i].set_title('Prediction = {}'.format(label_to_text[df_predict['emotion'][i]]))\n",
        "    axes[i].axis('off')\n",
        "    for j in range(1,31,2):\n",
        "            axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aaG4Hr_-xQ6"
      },
      "source": [
        "# EXCELLENT JOB! NOW YOU HAVE A SOLID KNOWLEDGE OF EMOTION AI! YOU SHOULD BE SUPER PROUD OF YOUR NEWLY ACQUIRED SKILLS :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BphUUmlbhF5M"
      },
      "source": [
        "# MINI CHALLENGE SOLUTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeVnCendfbWe"
      },
      "source": [
        "MINI CHALLENGE #1:\n",
        "- Obtain the average, minimum and maximum values for 'right_eye_center_x' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jugnf7fZffH9"
      },
      "source": [
        "keyfacial_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOG_GwZNg-fd"
      },
      "source": [
        "MINI CHALLENGE #2: \n",
        "- Perform a sanity check on on the data by randomly visualizing 64 new images along with their cooresponding key points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQCDpGJug_QL"
      },
      "source": [
        "import random\n",
        "# Let's view more images in a grid format\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(64):\n",
        "    k = random.randint(1, len(keyfacial_df))\n",
        "    ax = fig.add_subplot(8, 8, i + 1)\n",
        "    image = plt.imshow(keyfacial_df['Image'][k],cmap = 'gray')\n",
        "    for j in range(1,31,2):\n",
        "        plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], 'rx')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxKdCBQ6jrdr"
      },
      "source": [
        "MINI CHALLENGE #3:\n",
        "- Augment images by flipping them vertically \n",
        "(Hint: Flip along x-axis and note that if we are flipping along x-axis, x co-ordinates won't change)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wm2zsrEjr63"
      },
      "source": [
        "keyfacial_df_copy = copy.copy(keyfacial_df)\n",
        "\n",
        "# Flip the images along x axis\n",
        "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 0))\n",
        "\n",
        "# since we are flipping vertically, x coordinate values would be the same\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 1:\n",
        "    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHIqg0ej6Xz"
      },
      "source": [
        "MINI CHALLENGE #4:\n",
        "- Perform a sanity check and visualize sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4RQYra-j61j"
      },
      "source": [
        "# Show the vertically flipped image\n",
        "plt.imshow(keyfacial_df_copy['Image'][0],cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsoO8ffCkiNb"
      },
      "source": [
        "MINI CHALLENGE #5: \n",
        "- Try a different value for 'test_size' and verify that the split was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2dDhg5YkhTI"
      },
      "source": [
        "print('Train size =', X_train.shape)\n",
        "print('Test size =', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXeZ2X8alJOo"
      },
      "source": [
        "MINI CHALLENGE #6: \n",
        "- List at least 3 types of activation functions, which type is preferred in the hidden layers?\n",
        "- List at least 3 different neural networks architecutres and their applications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTNBK-DPmVyv"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1G8aJvO6IFxxd7dAdVB1tylWGNcSUM3R4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Rr8kHSmcot"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1d4HfwWTawWwpiXYfrADRUGLc0yfJNzoz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYglKx2BmjHz"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=142wHzaUyjv8185BZS-BisyjQALDOZhSX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_rqcj1moLo2"
      },
      "source": [
        "MINI CHALLENGE #7: \n",
        "- List 3 different neural networks architectures and their applications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucFdvm4xtQeq"
      },
      "source": [
        "Great Resource by Aravind Pai: https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE9T7Xi2ndbV"
      },
      "source": [
        "MINI CHALLENGE #8: \n",
        "- What happen when you set the learning rate to the extremes (i.e.: very small value and very large value)? How can you achieve the best of both worlds?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxQoOpStnsbT"
      },
      "source": [
        "Great Resource by Jason Brownlee: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_wClyTpmrLA"
      },
      "source": [
        "MINI CHALLENGE #9: \n",
        "- What's the performance of ResNets compared to AlexNet in ImageNet dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqf_KcNuA3o"
      },
      "source": [
        "https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWpMChuquTg5"
      },
      "source": [
        "MINI CHALLENGE #10:\n",
        "- Experiment with changing the network architecture by removing 2 MaxPooling layers from the Res Block and train the model\n",
        "- Try to add 'X = res_block(X, filter= [256,256,1024], stage= 4)' Block after stage #3 block. \n",
        "- What did you observe? Comment on your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAAPFVDqubNi"
      },
      "source": [
        "Number of trainable parameters increase significantly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFHS16-ZuzCh"
      },
      "source": [
        "MINI CHALLENGE #11:\n",
        "- Experiment with changing the batch size and validation split value and retrain the model (Take Home)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efnHqzQdvUu5"
      },
      "source": [
        "Take home! experiment with them and see if they improve the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iplZGYo1epx"
      },
      "source": [
        "MINI CHALLENGE #12: \n",
        "- Visualize the first image in the dataframe and make sure that the image is not distorted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gi1pl58uaXX"
      },
      "source": [
        "plt.imshow(facialexpression_df[' pixels'][0], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF-GHhFC2G5Z"
      },
      "source": [
        "MINI CHALLENGE #13: \n",
        "- Plot bar chart to outline how many samples (images) are present per emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAW2e0da2Agt"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts() )\n",
        "plt.title('Number of images per emotion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ZgLbiT3xey"
      },
      "source": [
        "MINI CHALLENGE #14: \n",
        "- Try at least 2 additional data augmentation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNEYRiiG39Bf"
      },
      "source": [
        "Check Keras documentation: https://keras.io/api/preprocessing/image/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elBWzl4G38CS"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "rotation_range = 15,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    shear_range = 0.1,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    brightness_range = [1.1, 1.5],\n",
        "    fill_mode = \"nearest\")\n",
        "\n",
        "# Note on \"Brightness_range\"\n",
        "# 1.0 does not affect image brightness\n",
        "# numbers less than 1.0 darken the image [0.5, 1.0]\n",
        "# numbers larger than 1.0 brighten the image [1.0, 1.5] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRhwEZUX6CxY"
      },
      "source": [
        "MINI CHALLENGE #15: \n",
        "- Experiment with various batch size, patience, optimizers, and network architecture to improve network performance (Take home)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hiFLFae6Gat"
      },
      "source": [
        "Take home! experiment and see if they improve the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MdDLWQS6-Cl"
      },
      "source": [
        "MINI CHALLENGE #16: \n",
        "- Print out a grid of 25 images along with their predicted/true label\n",
        "- Print out the classification report and analyze precision and recall "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyQCFI445KcQ"
      },
      "source": [
        "L = 5\n",
        "W = 5\n",
        "\n",
        "fig, axes = plt.subplots(L, W, figsize = (24, 24))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in np.arange(0, L*W):\n",
        "    axes[i].imshow(X_test[i].reshape(96,96), cmap = 'gray')\n",
        "    axes[i].set_title('Prediction = {}\\n True = {}'.format(label_to_text[predicted_classes[i]], label_to_text[y_true[i]]))\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace = 1)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jGZ0I70Fkf7"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, predicted_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvgvI057VEx"
      },
      "source": [
        "MINI CHALLENGE #17: \n",
        "- Plot a grid of 16 images along with their predicted emotion and facial key points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpHGScO7Vm5"
      },
      "source": [
        "# Plotting the test images and their predicted keypoints and emotions\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize = (24, 24))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(16):\n",
        "\n",
        "    axes[i].imshow(X_test[i].squeeze(),cmap='gray')\n",
        "    axes[i].set_title('Prediction = {}'.format(label_to_text[df_predict['emotion'][i]]))\n",
        "    axes[i].axis('off')\n",
        "    for j in range(1,31,2):\n",
        "            axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}